# -*- coding: utf-8 -*-
"""metrics_calculation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRK1RSqfUNG9HhSMZHmA-fMpLDHjOl9W

##CLIP directional similarities
"""

import torch
from transformers import (
    CLIPTokenizer,
    CLIPTextModelWithProjection,
    CLIPVisionModelWithProjection,
    CLIPImageProcessor,
)

clip_id = "openai/clip-vit-large-patch14"
tokenizer = CLIPTokenizer.from_pretrained(clip_id)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(device)
image_processor = CLIPImageProcessor.from_pretrained(clip_id)
image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(device)

import torch.nn as nn
import torch.nn.functional as F


class DirectionalSimilarity(nn.Module):
    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):
        super().__init__()
        self.tokenizer = tokenizer
        self.text_encoder = text_encoder
        self.image_processor = image_processor
        self.image_encoder = image_encoder

    def preprocess_image(self, image):
        image = self.image_processor(image, return_tensors="pt")["pixel_values"]
        return {"pixel_values": image.to(device)}

    def tokenize_text(self, text):
        inputs = self.tokenizer(
            text,
            max_length=self.tokenizer.model_max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {"input_ids": inputs.input_ids.to(device)}

    def encode_image(self, image):
        preprocessed_image = self.preprocess_image(image)
        image_features = self.image_encoder(**preprocessed_image).image_embeds
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        return image_features

    def encode_text(self, text):
        tokenized_text = self.tokenize_text(text)
        text_features = self.text_encoder(**tokenized_text).text_embeds
        text_features = text_features / text_features.norm(dim=1, keepdim=True)
        return text_features

    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):
        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)
        return sim_direction

    def forward(self, image_one, image_two, caption_one, caption_two):
        img_feat_one = self.encode_image(image_one)
        img_feat_two = self.encode_image(image_two)
        text_feat_one = self.encode_text(caption_one)
        text_feat_two = self.encode_text(caption_two)
        directional_similarity = self.compute_directional_similarity(
            img_feat_one, img_feat_two, text_feat_one, text_feat_two
        )
        return directional_similarity

from google.colab import drive
drive.mount('/content/drive')

import json
import os

ir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)

dataset_path = "/content/drive/MyDrive/counterfactual_dataset.json"
images_folder = "/content/drive/MyDrive/project_ii_images"

with open(dataset_path, 'r') as f:
    dataset = json.load(f)

scores = []
grouped_scores = []

from PIL import Image

for i, obj in enumerate(dataset):
    original_prompt = obj['original_prompt']
    counterfactual_prompt = obj['counterfactual_prompt']
    original_image = Image.open(f"{images_folder}/{obj['original_image']}")
    counterfactual_image = Image.open(f"{images_folder}/{obj['counterfactual_image']}")

    similarity_score = dir_similarity(
        original_image, counterfactual_image, original_prompt, counterfactual_prompt
    )
    scores.append(float(similarity_score.detach().cpu()))

scores_grouped = {
    "set1": scores[:5],
    "set2": scores[5:10],
    "set3": scores[10:15],
    "set4": scores[15:20],
    "set5": scores[20:25],
    "set6": scores[25:30],
    "set7": scores[30:35],
    "set8": scores[35:40],
    "set9": scores[40:45],
    "set10": scores[45:50],
}

output_json_path = "/content/drive/MyDrive/grouped_scores.json"

with open(output_json_path, 'w') as json_file:
    json.dump(scores_grouped, json_file, indent=4)

print(f"Grouped scores saved to {output_json_path}")

for i in scores_grouped.values():
    print(i)

"""##CLIP Score"""

!pip install torchmetrics

from diffusers import StableDiffusionPipeline
import torch
from torchmetrics.functional.multimodal import clip_score
from functools import partial
from PIL import Image
import numpy as np
import json
from torchvision.transforms import functional as F

model_ckpt = "CompVis/stable-diffusion-v1-4"
sd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to("cuda")

clip_score_fn = partial(clip_score, model_name_or_path="openai/clip-vit-base-patch16")

def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    image = F.to_tensor(image)
    return image.permute(1, 2, 0).numpy()

def calculate_clip_score(images, prompts):
    images_int = (images * 255).astype("uint8")
    clip_score_value = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()
    return round(float(clip_score_value), 4)

dataset_path = "/content/drive/MyDrive/counterfactual_dataset.json"
images_folder = "/content/drive/MyDrive/project_ii_images"

with open(dataset_path, 'r') as f:
    dataset = json.load(f)

counterfactual_prompts = []
counterfactual_images = []

for obj in dataset:
    counterfactual_prompts.append(obj['counterfactual_prompt'])
    image_path = f"{images_folder}/{obj['counterfactual_image']}"
    processed_image = preprocess_image(image_path)
    counterfactual_images.append(processed_image)

def calculate_clip_scores_per_image(images, prompts):
    scores = []
    for i, (image, prompt) in enumerate(zip(images, prompts)):
        image_int = (image * 255).astype("uint8")
        clip_score_value = clip_score_fn(
            torch.from_numpy(image_int).permute(2, 0, 1).unsqueeze(0),
            [prompt]
        ).detach().cpu().item()
        scores.append(round(clip_score_value, 4))
    return scores

clip_scores = calculate_clip_scores_per_image(counterfactual_images, counterfactual_prompts)

grouped_clip_scores = {
    f"group_{i // 5 + 1}": clip_scores[i:i + 5]
    for i in range(0, len(clip_scores), 5)
}

# Save grouped scores to JSON
output_grouped_scores_path = "/content/drive/MyDrive/grouped_clip_scores.json"
with open(output_grouped_scores_path, 'w') as f:
    json.dump(grouped_clip_scores, f, indent=4)

print(f"Grouped CLIP scores saved to {output_grouped_scores_path}")

grouped_clip_scores

all_scores = [score for group in grouped_clip_scores.values() for score in group]

average_score = sum(all_scores) / len(all_scores)

print(f"Average CLIP Score: {average_score}")

"""##Minimality (CLD)"""

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

model_name = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(model_name)
clip_processor = CLIPProcessor.from_pretrained(model_name)

images_folder = "/content/drive/MyDrive/project_ii_images"
dataset_path = "/content/drive/MyDrive/counterfactual_dataset.json"

with open(dataset_path, 'r') as f:
    dataset = json.load(f)

def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    return clip_processor(images=image, return_tensors="pt")

l2_cld_scores = []
cosine_cld_scores = []

for i, obj in enumerate(dataset):
    original_image_path = f"{images_folder}/{obj['original_image']}"
    counterfactual_image_path = f"{images_folder}/{obj['counterfactual_image']}"

    original_inputs = preprocess_image(original_image_path)
    counterfactual_inputs = preprocess_image(counterfactual_image_path)

    with torch.no_grad():
        original_latent = clip_model.get_image_features(**original_inputs)
        counterfactual_latent = clip_model.get_image_features(**counterfactual_inputs)

    l2_cld = torch.dist(original_latent, counterfactual_latent, p=2).item()
    l2_cld_scores.append(round(l2_cld, 4))

    cosine_cld = 1 - torch.nn.functional.cosine_similarity(original_latent, counterfactual_latent, dim=1).item()
    cosine_cld_scores.append(round(cosine_cld, 4))

cosine_cld_grouped = {
    f"group_{i // 5 + 1}": cosine_cld_scores[i:i + 5]
    for i in range(0, len(cosine_cld_scores), 5)
}

l2_cld_grouped = {
    f"group_{i // 5 + 1}": l2_cld_scores[i:i + 5]
    for i in range(0, len(l2_cld_scores), 5)
}

print(cosine_cld_grouped)

print(l2_cld_grouped)

output_l2_path = "/content/drive/MyDrive/l2_cld_grouped.json"
output_cosine_path = "/content/drive/MyDrive/cosine_cld_grouped.json"

with open(output_l2_path, 'w') as f:
    json.dump(l2_cld_grouped, f, indent=4)

with open(output_cosine_path, 'w') as f:
    json.dump(cosine_cld_grouped, f, indent=4)

print(f"L2 CLD scores saved to {output_l2_path}")
print(f"Cosine CLD scores saved to {output_cosine_path}")

"""##Realism (FID)"""

from torchmetrics.image.fid import FrechetInceptionDistance
from PIL import Image
import torch
import json
from torchvision.transforms import functional as F

images_folder = "/content/drive/MyDrive/project_ii_images"
dataset_path = "/content/drive/MyDrive/counterfactual_dataset.json"

with open(dataset_path, 'r') as f:
    dataset = json.load(f)

def preprocess_image(image_path):
    image = Image.open(image_path).convert("RGB")
    image_tensor = F.to_tensor(image)
    return image_tensor.unsqueeze(0)

def calculate_fid(real_images, fake_images):
    fid = FrechetInceptionDistance(normalize=True)
    fid.update(real_images, real=True)
    fid.update(fake_images, real=False)
    return float(fid.compute())

grouped_jsons = [
    dataset[i:i + 5] for i in range(0, len(dataset), 5)
]

grouped_fid_scores = {}

!pip install torchmetrics[image]

for group_idx, group in enumerate(grouped_jsons, start=1):
    real_images = []
    fake_images = []

    for obj in group:
        original_image_path = f"{images_folder}/{obj['original_image']}"
        counterfactual_image_path = f"{images_folder}/{obj['counterfactual_image']}"

        real_image = preprocess_image(original_image_path)
        fake_image = preprocess_image(counterfactual_image_path)

        real_images.append(real_image)
        fake_images.append(fake_image)

    real_images = torch.cat(real_images, dim=0) if real_images else None
    fake_images = torch.cat(fake_images, dim=0) if fake_images else None

    if real_images is not None and fake_images is not None:
        fid_score = calculate_fid(real_images, fake_images)
        grouped_fid_scores[f"group_{group_idx}"] = fid_score
        print(f"FID for group {group_idx}: {fid_score}")
    else:
        print(f"Skipping group {group_idx} due to missing images.")

output_fid_path = "/content/drive/MyDrive/grouped_fid_scores.json"
with open(output_fid_path, 'w') as f:
    json.dump(grouped_fid_scores, f, indent=4)

print(f"Grouped FID scores saved to {output_fid_path}")